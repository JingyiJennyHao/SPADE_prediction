
install.packages("LaplacesDemon")

library(parallel)

detectCores()


library(numDeriv)
library(LaplacesDemon)
library(MASS)
library(parallel)

# Set up parallel computing
n_cores <- 30  # Number of cores to use
cl <- makeCluster(n_cores)
clusterEvalQ(cl, {
  library(numDeriv)
  library(LaplacesDemon)
  library(MASS)
})
h <- function(p1, p2, sum_1, sum_2, sum_3) {
  result <- p1^sum_1 * p2^sum_2 * (1 - p1 - p2)^sum_3
  return(result)}

dataset <- function(sample_size){
  results_df <- data.frame(Ji_list = numeric(),
                           zi_list = numeric(),
                           pi1_list = numeric(),
                           pi2_list = numeric(),
                           sum1_list = numeric(),
                           sum2_list = numeric(),
                           sum3_list = numeric())
  
  for (i in 1:sample_size) {
    Ji <- round(10 * rweibull(1, shape = 1, scale = 1.5))
    Z_i <- sample(c(0, 1), 1)
    pi <- rdirichlet(1, c(1, 1, 1))
    pi1 <- pi[1, 1]
    pi2 <- pi[1, 2]
    
    pairs <- sample(c("(1,0)", "(0,1)", "(0,0)"), Ji, replace = TRUE, prob = c(pi1, pi2, 1 - pi1 - pi2))
    matrix_pairs <- matrix(as.numeric(unlist(strsplit(gsub("\\(|\\)", "", pairs), ","))), ncol = 2, byrow = TRUE)
    Dij_1 <- matrix_pairs[, 1]
    Dij_2 <- matrix_pairs[, 2]
    
    sum1 <- sum(Dij_1)
    sum2 <- sum(Dij_2)
    sum3 <- sum((1 - Dij_1) * (1 - Dij_2))
    
    # Append the values to the data frame
    results_df <- rbind(results_df, data.frame(Ji_list = Ji, pi1_list = pi1, pi2_list = pi2, sum1_list = sum1, sum2_list = sum2, sum3_list = sum3))
  }
  
  return(results_df)
}

fi <- function(params, sum_1, sum_2, sum_3) {
  n_pairs <- 150
  pi_mc <- rdirichlet(n_pairs, params)
  p1_mc <- pi_mc[, 1]
  p2_mc <- pi_mc[, 2]
  mc_list <- numeric(n_pairs)
  for (i in 1:n_pairs) {
    mc_list[i] <- h(p1_mc[i], p2_mc[i], sum_1, sum_2, sum_3)
  }
  mean_result <- mean(mc_list)
  return(log(mean_result))
}

llh <- function(log_params, data) {
  params <- exp(log_params)
  func <- 0
  sample_size <- nrow(data)
  
  for (i in 1:sample_size) {
    # Extract data from the provided data frame
    pi1 <- data$pi1_list[i]
    pi2 <- data$pi2_list[i]
    sum_1 <- data$sum1_list[i]
    sum_2 <- data$sum2_list[i]
    sum_3 <- data$sum3_list[i]
    
    # Calculate individual likelihood llh_i
    llh_i <- fi(params, sum_1, sum_2, sum_3)
    
    # if (is.infinite(llh_i)) {
    #   llh_i <- 0
    #   }
    
    # Accumulate individual likelihoods
    func <- func + llh_i
  }
  return(-func)
}

g <- function(x,alpha){
  log_ddirichlet <- function(x, alpha) {
    return(ddirichlet(x, alpha, log=TRUE))
  }
  gradient <- grad(function(a) log_ddirichlet(x, a), alpha)
  return(gradient)
}

fisher_info <- function(alpha_hat, dataset){
  sample_size <- nrow(dataset)
  n_pairs <- 150
  var_matrix <- matrix(data=NA,nrow=3,ncol=sample_size)
  
  
  for (i in 1:sample_size){
    pi_mc <- rdirichlet(n_pairs, alpha_hat)
    p1_mc <- pi_mc[, 1]
    p2_mc <- pi_mc[, 2]
    sum_1 <- dataset$sum1_list[i]
    sum_2 <- dataset$sum2_list[i]
    sum_3 <- dataset$sum3_list[i]
    mc_list_1 <- numeric(n_pairs)
    
    for (j in 1:n_pairs) {
      mc_list_1[j] <- h(p1_mc[j], p2_mc[j], sum_1, sum_2, sum_3)
    }
    temp1 <- mean(mc_list_1)
    
    pi_mc <- rdirichlet(n_pairs, alpha_hat)
    p1_mc <- pi_mc[, 1]
    p2_mc <- pi_mc[, 2]
    mc_list_2 <- matrix(data=NA,nrow=3,ncol=n_pairs)
    for (j in 1:n_pairs) {
      x <- c(p1_mc[j],p2_mc[j],1-p1_mc[j]-p2_mc[j])
      mc_list_2[, j] <- h(p1_mc[j],p2_mc[j],sum_1, sum_2, sum_3)*g(x, alpha_hat)
    }
    
    temp2 <- rowMeans(mc_list_2)
    var_matrix[,i]<-temp2/temp1
  }
  
  cov_matrix <- cov(t(var_matrix))
  return(cov_matrix)
}
# Export necessary functions and data to the cluster
clusterExport(cl, c("h", "dataset", "fi", "llh", "g", "fisher_info"))

# Function to calculate fi
calc_fi <- function(params, sum_1, sum_2, sum_3) {
  n_pairs <- 150
  pi_mc <- parLapply(cl, 1:n_pairs, function(i) LaplacesDemon::rdirichlet(1, params)[, 1:2])
  p1_mc <- sapply(pi_mc, function(x) x[, 1])
  p2_mc <- sapply(pi_mc, function(x) x[, 2])
  mc_list <- parLapply(cl, 1:n_pairs, function(i) h(p1_mc[[i]], p2_mc[[i]], sum_1, sum_2, sum_3))
  mean_result <- mean(unlist(mc_list))
  return(log(mean_result))
}

# Function to calculate the log-likelihood
calc_llh <- function(log_params, data) {
  params <- exp(log_params)
  func <- parLapply(cl, 1:nrow(data), function(i) {
    pi1 <- data$pi1_list[i]
    pi2 <- data$pi2_list[i]
    sum_1 <- data$sum1_list[i]
    sum_2 <- data$sum2_list[i]
    sum_3 <- data$sum3_list[i]
    fi(params, sum_1, sum_2, sum_3)
  })
  return(-sum(unlist(func)))
}


start_time <- Sys.time()
# Use parLapply to parallelize the replicate loop
bias <- numeric(3)
emp_se <- numeric(3)
mse <- numeric(3)
coverage <- numeric(3)
replicate <- 50

for (rep in 1:replicate) {
  sample_size <- 300
  simulated_dataset <- dataset(sample_size)
  log_initial_params <- log(c(1, 1, 1))
  result <- suppressWarnings(optim(par = log_initial_params, fn = calc_llh, method = "BFGS", data = simulated_dataset))
  params_hat <- exp(result$par)
  fisher_matrix <- fisher_info(params_hat, simulated_dataset)
  sigma <- solve(fisher_matrix) / sample_size
  z <- qnorm(0.975)
  SE <- sqrt(diag(sigma))
  lower_ci <- params_hat - z * SE
  upper_ci <- params_hat + z * SE
  
  # True parameters (assuming you know them)
  true_params <- c(1, 1, 1)
  
  # Calculate Bias
  bias <- bias + params_hat - true_params
  
  # Calculate EmpSE
  emp_se <- emp_se + SE
  
  # Calculate MSE
  mse <- mse + (params_hat - true_params)^2
  
  # Calculate coverage
  coverage <- coverage + (lower_ci <= true_params & true_params <= upper_ci)
}

# Stop the parallel cluster
stopCluster(cl)

# Calculate average Bias, EmpSE, MSE, and coverage
bias <- bias / replicate
emp_se <- emp_se / replicate
mse <- mse / replicate
coverage <- coverage / replicate  # Convert to percentage

end_time <- Sys.time()
print(bias)
print(emp_se)
print(mse)
print(coverage)

print(end_time - start_time)
